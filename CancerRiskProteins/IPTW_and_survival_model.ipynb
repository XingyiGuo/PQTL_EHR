{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775bd2b1-46b6-435b-9df9-7c6dabd6354c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install tqdm swifter openpyxl lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed32fc4-6edf-4d69-a975-889d95a7a129",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation, plotting, and statistical analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import textwrap\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Machine Learning and statistical modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import survival_difference_at_fixed_point_in_time_test\n",
    "from lifelines.exceptions import ConvergenceError, StatisticalWarning\n",
    "\n",
    "# Importing libraries for handling complex operations efficiently\n",
    "import swifter\n",
    "import openpyxl\n",
    "import numpy.linalg as la\n",
    "\n",
    "# Configuration to handle warnings in the code\n",
    "import warnings\n",
    "# Ignore specific statistical warnings from lifelines library\n",
    "warnings.filterwarnings('ignore', category=StatisticalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab12a2c4-8735-4558-b392-410c8531a6e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_atc_codes(atc_name, n_level, atc_data):\n",
    "    \"\"\"\n",
    "    Given an ATC name, find all ATC codes where the ATC name is a substring of the ATC names in the data.\n",
    "    The function then filters these codes to match a specific ATC level and have a length of 7,\n",
    "    excluding the ATC code of the given name.\n",
    "\n",
    "    Args:\n",
    "        atc_name (str): A substring of the ATC name to match.\n",
    "        n_level (int): The level of the ATC code to match, maximum is 4.\n",
    "        atc_data (pd.DataFrame): The DataFrame containing ATC codes and names.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of ATC names.\n",
    "    \"\"\"\n",
    "    # Define the length for each ATC level\n",
    "    atc_length = {1: 1, 2: 3, 3: 4, 4: 5}\n",
    "\n",
    "    # Validate the n_level parameter\n",
    "    if n_level not in atc_length:\n",
    "        raise ValueError(\"Invalid ATC level. Please choose a level between 1 and 4.\")\n",
    "\n",
    "    # Find ATC codes where the given ATC name is a substring of the ATC names\n",
    "    matching_atc_codes = atc_data[atc_data['atc_name'].str.lower() == atc_name.lower()]['atc_code']\n",
    "\n",
    "    # Initialize an empty list for storing similar ATC names\n",
    "    similar_atc_names = []\n",
    "\n",
    "    for atc_code in matching_atc_codes:\n",
    "        # Extract the first characters according to the ATC level\n",
    "        first_chars = atc_code[:atc_length[n_level]]\n",
    "\n",
    "        # Filter for ATC codes with the specified first characters and length of 7\n",
    "        similar_atc_codes_df = atc_data[(atc_data['atc_code'].str.startswith(first_chars)) & \n",
    "                                        (atc_data['atc_code'].str.len() == 7) &\n",
    "                                        (atc_data['atc_code'] != atc_code)]\n",
    "\n",
    "        # Append the ATC names to the list\n",
    "        similar_atc_names.extend(similar_atc_codes_df['atc_name'].tolist())\n",
    "    \n",
    "    return list(set(similar_atc_names))\n",
    "\n",
    "def sep_icd_code(x):\n",
    "    \"\"\"\n",
    "    Separates ICD (International Classification of Diseases) codes from a given string.\n",
    "    \n",
    "    The function expects a string containing ICD codes followed by descriptions,\n",
    "    separated by new lines. It extracts and returns the first element (the ICD code)\n",
    "    from each line.\n",
    "    \n",
    "    Parameters:\n",
    "    - x (str): A string containing ICD codes and their descriptions, separated by new lines.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of ICD codes extracted from the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the input string into a list of substrings based on new line character.\n",
    "    substrings = x.split('\\n')\n",
    "    \n",
    "    # Extract the first element (ICD code) from each substring.\n",
    "    # Each ICD code is followed by a space and then the description.\n",
    "    first_elements = [substring.split(' ')[0] for substring in substrings]\n",
    "    \n",
    "    return first_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2823757c-adce-46b4-aa17-ac68f29df9ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SQLTable:\n",
    "    \"\"\"\n",
    "    Query from RD database from VUMC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_name, items=None, category=None):\n",
    "        \"\"\"Initialize SQLTable with table name, items, and category.\"\"\"\n",
    "        self.table_name = table_name\n",
    "        self.items = items\n",
    "        self.category = category\n",
    "\n",
    "    def format_query_items(self):\n",
    "        \"\"\"Format query items based on the category.\n",
    "\n",
    "        - For 'drug' category, format items for drug source value.\n",
    "        - For 'cancer' category, format items for ICD10 and ICD9 codes.\n",
    "        - For 'phecode' category, format items for ICD10 and ICD9 codes.\n",
    "        - Return None for other categories.\n",
    "        \"\"\"\n",
    "        if self.category == 'drug':\n",
    "            query_string = \" OR\\n                        \".join(\n",
    "                f'LOWER(c.concept_name) LIKE \"%{item.lower()}%\"' for item in self.items\n",
    "            )\n",
    "            return query_string\n",
    "        elif self.category == 'cancer':\n",
    "            query_string_icd10 = \" OR\\n                                    \".join(\n",
    "                f'concept_code LIKE \"{item}%\"' for item in self.items['icd10']\n",
    "            )\n",
    "            query_string_icd9 = \" OR\\n                                    \".join(\n",
    "                f'concept_code LIKE \"{item}%\"' for item in self.items['icd9']\n",
    "            )\n",
    "            return query_string_icd10, query_string_icd9\n",
    "        elif self.category == 'phecode':\n",
    "            query_string_icd10 = \", \".join(f\"'{item}'\" for item in self.items['icd10'])\n",
    "            query_string_icd9 = \", \".join(f\"'{item}'\" for item in self.items['icd9'])\n",
    "            return query_string_icd10, query_string_icd9\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def drop_table(self):\n",
    "        \"\"\"Drop the SQL table if it exists.\"\"\"\n",
    "        if isinstance(self.table_name, list):\n",
    "            spark.sql(f'DROP TABLE IF EXISTS {self.table_name[0]}')\n",
    "        else:\n",
    "            spark.sql(f'DROP TABLE IF EXISTS {self.table_name}')\n",
    "\n",
    "    def create_table(self):\n",
    "        \"\"\"Create an SQL table based on the category.\n",
    "\n",
    "        Different SQL queries are generated for different categories\n",
    "        such as 'drug', 'cancer', 'record', 'demo', and 'phecode'.\n",
    "        \"\"\"\n",
    "        self.drop_table()\n",
    "\n",
    "        if self.category == 'drug':\n",
    "            # For 'drug' category\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name} (\n",
    "                    SELECT\n",
    "                        DISTINCT de.person_id, de.drug_exposure_start_date \n",
    "                    FROM \n",
    "                        rd_omop_prod.drug_exposure de\n",
    "                    JOIN\n",
    "                        rd_omop_prod.drug_strength ds\n",
    "                    ON \n",
    "                        de.drug_concept_id = ds.drug_concept_id\n",
    "                    JOIN \n",
    "                        rd_omop_prod.concept c\n",
    "                    ON \n",
    "                        c.concept_id = ds.ingredient_concept_id\n",
    "                    WHERE (\n",
    "                        {self.format_query_items()}\n",
    "                    ) \n",
    "                )\n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "        \n",
    "        elif self.category == 'cancer':\n",
    "            # For 'cancer' category\n",
    "            string_icd10, string_icd9 = self.format_query_items()\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name} (\n",
    "                    SELECT\n",
    "                        person_id, MIN(condition_start_date) AS cancer_start_date\n",
    "                    FROM\n",
    "                        rd_omop_prod.condition_occurrence\n",
    "                    WHERE \n",
    "                        condition_source_concept_id IN (\n",
    "                            SELECT concept_id\n",
    "                            FROM rd_omop_prod.concept\n",
    "                            WHERE (\n",
    "                                    ({string_icd10})\n",
    "                                AND vocabulary_id = 'ICD10CM')\n",
    "                            OR (\n",
    "                                    ({string_icd9})\n",
    "                                AND vocabulary_id = 'ICD9CM')\n",
    "                        )\n",
    "                    GROUP BY person_id\n",
    "                )\n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "        \n",
    "        # Handle the 'record' category\n",
    "        elif self.category == 'record':\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name[0]} (\n",
    "                    SELECT\n",
    "                        person_id, MIN(condition_start_date) AS record_start_date, \n",
    "                        MAX(condition_start_date) AS record_end_date\n",
    "                    FROM\n",
    "                        rd_omop_prod.condition_occurrence\n",
    "                    WHERE person_id IN (\n",
    "                        SELECT person_id FROM {self.table_name[1]}\n",
    "                        UNION\n",
    "                        SELECT person_id FROM {self.table_name[2]}\n",
    "                    )\n",
    "                    GROUP BY person_id\n",
    "                )                       \n",
    "            ''')\n",
    "\n",
    "            spark.sql(sql_query)\n",
    "\n",
    "        # Handle the 'demo' category\n",
    "        elif self.category == 'demo':\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name[0]} (\n",
    "                    WITH table1 AS (\n",
    "                        SELECT \n",
    "                            person_id, \n",
    "                            birth_datetime,\n",
    "                            gender_source_value, \n",
    "                            ethnicity_concept_id, \n",
    "                            race_concept_id\n",
    "                        FROM \n",
    "                            rd_omop_prod.person\n",
    "                        WHERE person_id IN (\n",
    "                            SELECT person_id FROM {self.table_name[1]}\n",
    "                            UNION\n",
    "                            SELECT person_id FROM {self.table_name[2]}\n",
    "                        )\n",
    "                    ),\n",
    "                    table2 as (\n",
    "                        SELECT\n",
    "                            table1.*, \n",
    "                            c.concept_name AS race_source_value\n",
    "                        FROM table1\n",
    "                        LEFT JOIN\n",
    "                            rd_omop_prod.concept c ON table1.race_concept_id = c.concept_id\n",
    "                    )\n",
    "                    SELECT\n",
    "                        table2.person_id, table2.birth_datetime, table2.gender_source_value, \n",
    "                        table2.race_source_value, c.concept_name AS ethnicity_source_value\n",
    "                    FROM table2\n",
    "                    LEFT JOIN\n",
    "                        rd_omop_prod.concept c ON table2.ethnicity_concept_id = c.concept_id\n",
    "                    WHERE table2.birth_datetime < GETDATE()\n",
    "                )\n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "\n",
    "        # Handle the 'death' category\n",
    "        elif self.category == 'death':\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name[0]} (\n",
    "                    SELECT \n",
    "                        person_id, death_date, cause_source_value, cause_source_concept_id\n",
    "                    FROM \n",
    "                        rd_omop_prod.death\n",
    "                    WHERE person_id IN (\n",
    "                        SELECT person_id FROM {self.table_name[1]}\n",
    "                        UNION\n",
    "                        SELECT person_id FROM {self.table_name[2]}\n",
    "                    )\n",
    "                )                        \n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "        \n",
    "        # Handle the 'drug_count' category\n",
    "        elif self.category == 'drug_count':\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name} (\n",
    "                    WITH table1 AS (\n",
    "                        SELECT \n",
    "                            ingredient_concept_id, \n",
    "                            COUNT(DISTINCT de.person_id) AS count_person_id\n",
    "                        FROM rd_omop_prod.drug_exposure de\n",
    "                        JOIN rd_omop_prod.drug_strength ds\n",
    "                        ON de.drug_concept_id = ds.drug_concept_id\n",
    "                        GROUP BY ingredient_concept_id\n",
    "                    )\n",
    "\n",
    "                    SELECT \n",
    "                        LOWER(c.concept_name) AS ingredient_name,\n",
    "                        table1.count_person_id\n",
    "                    FROM table1\n",
    "                    JOIN rd_omop_prod.concept c\n",
    "                    ON table1.ingredient_concept_id = c.concept_id\n",
    "                )         \n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "            \n",
    "        # Handle the 'phecode' category\n",
    "        elif self.category == 'phecode':\n",
    "            string_icd10, string_icd9 = self.format_query_items()\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name}( \n",
    "                    WITH table1 AS (\n",
    "                        SELECT \n",
    "                            DISTINCT phecode\n",
    "                        FROM \n",
    "                            rd_omop_prod.x_phecodes_icd_mapping\n",
    "                        WHERE \n",
    "                            (icd in ({string_icd9}) AND icd_flag = 9)\n",
    "                        OR\n",
    "                            ((icd in ({string_icd10}) AND icd_flag = 10))    \n",
    "                    ),\n",
    "                    table2 AS (\n",
    "                        SELECT \n",
    "                            DISTINCT p.person_id, o.condition_start_date, o.condition_source_concept_id\n",
    "                        FROM\n",
    "                            rd_omop_prod.condition_occurrence o\n",
    "                        JOIN\n",
    "                            pat_cohort p\n",
    "                        ON\n",
    "                            o.person_id = p.person_id\n",
    "                        WHERE\n",
    "                            o.condition_start_date <= p.phecode_end_date\n",
    "                    )\n",
    "\n",
    "                    SELECT \n",
    "                        DISTINCT t2.person_id, t2.condition_start_date, cp.phecode\n",
    "                    FROM \n",
    "                        table2 t2\n",
    "                    JOIN \n",
    "                        (SELECT\n",
    "                            c.concept_id, p.phecode\n",
    "                        FROM \n",
    "                            rd_omop_prod.x_phecodes_icd_mapping p\n",
    "                        JOIN \n",
    "                            table1 t1\n",
    "                        ON \n",
    "                            t1.phecode = p.phecode\n",
    "                        JOIN\n",
    "                            rd_omop_prod.concept c\n",
    "                        ON \n",
    "                            p.ICD = c.concept_code\n",
    "                            AND \n",
    "                            ((c.vocabulary_id = 'ICD10CM' AND p.icd_flag = 10)\n",
    "                                OR (c.vocabulary_id = 'ICD9CM' AND p.icd_flag = 9))) cp\n",
    "                    ON \n",
    "                        t2.condition_source_concept_id = cp.concept_id\n",
    "                )                     \n",
    "            ''')\n",
    "            spark.sql(sql_query)                            \n",
    "        # Handle the 'bmi' category\n",
    "        elif self.category == 'bmi':\n",
    "            sql_query = textwrap.dedent(f'''\n",
    "                CREATE TABLE {self.table_name}( \n",
    "                WITH table1 AS (\n",
    "                SELECT \n",
    "                    DISTINCT p.person_id, \n",
    "                    m.value_as_number AS bmi, \n",
    "                    m.measurement_date,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY p.person_id ORDER BY m.measurement_date DESC) AS rank\n",
    "                FROM \n",
    "                    rd_omop_prod.x_vs_bmi_clean b\n",
    "                JOIN \n",
    "                    pat_cohort p\n",
    "                ON \n",
    "                    b.person_id = p.person_id\n",
    "                JOIN \n",
    "                    rd_omop_prod.measurement m\n",
    "                ON \n",
    "                    b.measurement_id = m.measurement_id\n",
    "                WHERE \n",
    "                    m.measurement_date <= p.phecode_end_date\n",
    "                AND \n",
    "                    b.x_is_cleaned = 'Y'\n",
    "                )\n",
    "\n",
    "                SELECT\n",
    "                    person_id, bmi\n",
    "                FROM\n",
    "                    table1\n",
    "                WHERE\n",
    "                    rank = 1\n",
    "                )                     \n",
    "            ''')\n",
    "            spark.sql(sql_query)\n",
    "\n",
    "        # Placeholder for other categories\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d74696-5ed4-40c6-8c84-1f07b43f849f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, controlled_drugs, cancer_icd_codes):\n",
    "        \"\"\"Initialize the Dataset class with drug/cancer data.\n",
    "\n",
    "        Args:\n",
    "            controlled_drugs (list/dict): List or dictionary of controlled drugs.\n",
    "            cancer_icd_codes (list/dict): List or dictionary of cancer ICD codes.\n",
    "        \"\"\"\n",
    "        self.controlled_drugs = controlled_drugs\n",
    "        self.cancer_icd_codes = cancer_icd_codes\n",
    "\n",
    "    @staticmethod\n",
    "    def spark_to_df(table_name):\n",
    "        \"\"\"Convert a Spark SQL table to a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to convert.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Pandas DataFrame containing the data from the Spark SQL table.\n",
    "        \"\"\"\n",
    "        df = spark.sql(f'SELECT * FROM {table_name}').toPandas()\n",
    "        return df\n",
    "\n",
    "    def collect_data(self):\n",
    "        \"\"\"Collect data from various SQL tables and convert them to DataFrames.\n",
    "\n",
    "        Creates tables for treated drugs, controlled drugs, and cancer ICD codes,\n",
    "        and then converts these tables into Pandas DataFrames.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple of DataFrames for control group, case group, patient records, \n",
    "                   cancer patients, and demographic data.\n",
    "        \"\"\"\n",
    "        # Create SQL tables for the different datasets\n",
    "        SQLTable(table_name='pat_group1', items=self.controlled_drugs, category='drug').create_table()\n",
    "        SQLTable(table_name='pat_cancer', items=self.cancer_icd_codes, category='cancer').create_table()\n",
    "        SQLTable(table_name=['pat_record', 'pat_group1', 'pat_group2'], category='record').create_table()\n",
    "        SQLTable(table_name=['pat_demo', 'pat_group1', 'pat_group2'], category='demo').create_table()\n",
    "        \n",
    "\n",
    "        # Convert the SQL tables to Pandas DataFrames\n",
    "        pat_control = self.spark_to_df('pat_group1')\n",
    "        pat_record = self.spark_to_df('pat_record')\n",
    "        pat_cancer = self.spark_to_df('pat_cancer')\n",
    "        pat_demo = self.spark_to_df('pat_demo')\n",
    "\n",
    "        return pat_control, pat_record, pat_cancer, pat_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2f9597-147b-4443-9119-b2ef2270fcf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, pat_demo, pat_case, pat_control, pat_cancer, pat_record):\n",
    "        \"\"\"Initializes the DataProcessor class with necessary datasets.\"\"\"\n",
    "        self.pat_demo = pat_demo\n",
    "        self.pat_case = pat_case\n",
    "        self.pat_control = pat_control\n",
    "        self.pat_cancer = pat_cancer\n",
    "        self.pat_record = pat_record\n",
    "    \n",
    "    @staticmethod\n",
    "    def aggregate_drug_record(df):\n",
    "        \"\"\"\n",
    "        - Excludes false records based on drug exposure start date.\n",
    "        - Aggregates data to find the first drug exposure dates and count of record within 36 months.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The patient record DataFrame to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The preprocessed patient record DataFrame.\n",
    "        \"\"\"\n",
    "        df['drug_exposure_start_date'] = pd.to_datetime(df['drug_exposure_start_date'])\n",
    "        df = df[df['drug_exposure_start_date'] > datetime.strptime('1990-01-01', '%Y-%m-%d')]\n",
    "        \n",
    "        if len(df) > 1:\n",
    "\n",
    "            # Find the minimum drug_exposure_start_date for each person_id\n",
    "            min_dates = df.groupby('person_id')['drug_exposure_start_date'].min().reset_index(name='drug_start_date')\n",
    "\n",
    "            # Join this information back to the original DataFrame\n",
    "            df_with_start = pd.merge(df, min_dates, on='person_id')\n",
    "\n",
    "            # Define a function to count records within 36 months from the drug_start_date\n",
    "            def count_within_36_months(group):\n",
    "                # Calculate the end date as 36 months after the drug_start_date\n",
    "                group['end_date'] = group['drug_start_date'] + pd.DateOffset(months=36)\n",
    "                # Count how many records fall within the start and end date\n",
    "                return group[(group['drug_exposure_start_date'] >= group['drug_start_date']) & \n",
    "                            (group['drug_exposure_start_date'] <= group['end_date'])].shape[0]\n",
    "\n",
    "            # Apply the function to each group and reset the index\n",
    "            counts = df_with_start.groupby('person_id').apply(count_within_36_months).reset_index(name='record_counts')\n",
    "\n",
    "            # Combine the counts with the min_dates to have the final DataFrame\n",
    "            result_df = pd.merge(min_dates, counts, on='person_id')\n",
    "\n",
    "            return result_df[result_df['record_counts'] > 1].reset_index()\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    def merge_data(self):\n",
    "        \"\"\"Merges various datasets and performs additional preprocessing for the final analysis.\"\"\"\n",
    "        # Sets flags in the demographic data to indicate case, control, and cancer status.\n",
    "        self.pat_demo['is_case'] = self.pat_demo.person_id.isin(self.pat_case.person_id)\n",
    "        self.pat_demo['is_control'] = self.pat_demo.person_id.isin(self.pat_control.person_id)\n",
    "        self.pat_demo['is_cancer'] = self.pat_demo.person_id.isin(self.pat_cancer.person_id)\n",
    "\n",
    "        # Exclude patients who took both drugs\n",
    "        df = self.pat_demo[(~self.pat_demo['is_case'] & self.pat_demo['is_control']) | (self.pat_demo['is_case'] & ~self.pat_demo['is_control'])]\n",
    "\n",
    "        # Merging data\n",
    "        df = pd.merge(df, pd.concat([self.pat_case, self.pat_control]), on='person_id', how='inner')\n",
    "        df = pd.merge(df, self.pat_cancer, on='person_id', how='left')\n",
    "        df = pd.merge(df, self.pat_record, on='person_id', how='left')\n",
    "        df['race_source_value'] = df['race_source_value'].apply(lambda x: self.map_race_value(x))\n",
    "        \n",
    "        # Drop patient with no icd code record\n",
    "        df = df.dropna(subset='record_start_date')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def map_race_value(x):\n",
    "        \"\"\"Maps race values to categories.\"\"\"\n",
    "        if x not in ['White', 'Black or African American', 'No matching concept']:\n",
    "            return 'Other'\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def calculate_variables(self, df):\n",
    "        \"\"\"Calculates various variables for analysis.\"\"\"\n",
    "\n",
    "        df['drug_start_date'] = pd.to_datetime(df['drug_start_date'])\n",
    "        df['cancer_start_date'] = pd.to_datetime(df['cancer_start_date'])\n",
    "        df['record_start_date'] = pd.to_datetime(df['record_start_date'])\n",
    "        df['record_end_date'] = pd.to_datetime(df['record_end_date'])\n",
    "\n",
    "        # Baseline period calculations\n",
    "        df['drug_baseline_period'] = self.calculate_time_period(df, 'drug_start_date', 'record_start_date')\n",
    "        df['cancer_baseline_period'] = self.calculate_time_period(df, 'cancer_start_date', 'record_start_date')\n",
    "        df['record_baseline_period'] = self.calculate_time_period(df, 'record_end_date', 'record_start_date')\n",
    "        df['time_to_event'] = self.calculate_time_period(df, 'cancer_start_date', 'drug_start_date')\n",
    "        \n",
    "        # Require at least 12 months record before first drug exposure or diagnose the cancer as well as at least 12 months record length \n",
    "        df = df[(df['drug_baseline_period'] >= 12) & (df['record_baseline_period'] >= 12)]\n",
    "        df = df[df.apply(lambda x: x.cancer_baseline_period >= 15 if x.is_cancer else True, axis=1)]\n",
    "\n",
    "        # Calculates ages at different time points.\n",
    "        df['birth_date'] = pd.to_datetime(df.birth_datetime)\n",
    "        df['cancer_age'] = self.calculate_time_period(df, 'cancer_start_date', 'birth_date')\n",
    "        df['record_end_age'] = self.calculate_time_period(df, 'record_end_date', 'birth_date')\n",
    "\n",
    "        # Require cancer age >= 40 or last record age >= 40\n",
    "        df['age'] = df.apply(lambda x: x.cancer_age if x.is_cancer else x.record_end_age, axis=1)\n",
    "        df = df[df['age'] >= 40 * 12]\n",
    "        df['phecode_end_date'] = df.apply(lambda x: x.cancer_start_date - pd.DateOffset(months=3) if x.is_cancer else x.record_end_date, axis=1)\n",
    "        df['phecode_end_date'] = df['phecode_end_date'].dt.date\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_time_period(x, column_name_1, column_name_2):\n",
    "        \"\"\"Calculates the time period for two given columns.\"\"\"\n",
    "        period = (x[column_name_1] - x[column_name_2])\n",
    "        return period.apply(lambda x: x.days / 30.4375 if not pd.isna(x) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3442b9de-7d6c-4103-a04f-9f22b05d09fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PropensityScoreLR:\n",
    "    def __init__(self, confounder, treatment, random_seed):\n",
    "        \"\"\"\n",
    "        Initializes the PropensityScoreLR with data and treatment label column.\n",
    "\n",
    "        Args:\n",
    "            confounder (np.array): The confounder variables.\n",
    "            treatment (np.array): The treatment labels.\n",
    "            random_seed (int): The random state used in cross validation split and logistic regression\n",
    "        \"\"\"\n",
    "\n",
    "        self.confounder = confounder\n",
    "        self.treatment = treatment\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate(array):\n",
    "        \"\"\" Truncates values in an array to the specified lower and upper percentiles. \"\"\"\n",
    "        # lower_value = np.percentile(array, 1)\n",
    "        # upper_value = np.percentile(array, 99)\n",
    "        # return np.clip(array, lower_value, upper_value)\n",
    "        return np.clip(array, a_min=5e-06, a_max=5e1)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def weighted_mean(x, w):\n",
    "        \"\"\" Calculate weighted mean. \"\"\"\n",
    "        return np.sum(np.multiply(x, w), axis=0) / w.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_var(x, w):\n",
    "        \"\"\" Calculate weighted variance. \"\"\"\n",
    "        m_w = PropensityScoreLR.weighted_mean(x, w)\n",
    "        nw, nsw = w.sum(), (w ** 2).sum()\n",
    "        var = np.multiply((x - m_w) ** 2, w)\n",
    "        return np.sum(var, axis=0) * (nw / (nw ** 2 - nsw))\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_IPTW(y, ps):\n",
    "        \"\"\" Calculate Inverse Probability of Treatment Weights. \"\"\"\n",
    "        ones_idx, zeros_idx = np.where(y == True), np.where(y == False)\n",
    "        p_T = len(ones_idx[0]) / (len(ones_idx[0]) + len(zeros_idx[0]))\n",
    "        treated_w = p_T / ps[ones_idx]\n",
    "        controlled_w = (1 - p_T) / (1. - ps[zeros_idx])\n",
    "        treated_w, controlled_w = PropensityScoreLR.truncate(treated_w), PropensityScoreLR.truncate(controlled_w)\n",
    "        return np.reshape(treated_w, (len(treated_w), 1)), np.reshape(controlled_w, (len(controlled_w), 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_SMD(X, y, propensity_score):\n",
    "        \"\"\" Calculate Standardized Mean Differences. \"\"\"\n",
    "        ones_idx, zeros_idx = np.where(y == True), np.where(y == False)\n",
    "        treated_w, controlled_w = PropensityScoreLR.cal_IPTW(y, propensity_score)\n",
    "        treated_X, controlled_X = X[ones_idx], X[zeros_idx]\n",
    "\n",
    "        treated_X_w_mu = PropensityScoreLR.weighted_mean(treated_X, treated_w)\n",
    "        controlled_X_w_mu = PropensityScoreLR.weighted_mean(controlled_X, controlled_w)\n",
    "        treated_X_w_var = PropensityScoreLR.weighted_var(treated_X, treated_w)\n",
    "        controlled_X_w_var = PropensityScoreLR.weighted_var(controlled_X, controlled_w)\n",
    "        VAR = np.sqrt((treated_X_w_var + controlled_X_w_var) / 2)\n",
    "        SMD = np.divide(np.abs(treated_X_w_mu - controlled_X_w_mu), VAR, \n",
    "        out=np.zeros_like(treated_X_w_mu), where=(VAR!=0))\n",
    "        return SMD\n",
    "\n",
    "    def n_weight(self, estimator, X, y):\n",
    "        \"\"\" Custom evaluation function: count of unbalanced weighted covariate. \"\"\"\n",
    "\n",
    "        propensity_score = estimator.predict_proba(self.confounder)[:, 1]\n",
    "        SMD = self.cal_SMD(self.confounder, self.treatment, propensity_score)\n",
    "        ps = estimator.predict_proba(X)[:, 1]\n",
    "        auc = roc_auc_score(y, ps)\n",
    "        return len(np.where(SMD <= 0.1)[0]) + auc\n",
    "\n",
    "    def fit_model(self):\n",
    "        \"\"\" Fit the logistic regression model using GridSearchCV. \"\"\"\n",
    "        model = LogisticRegression()\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=self.random_seed)\n",
    "        param_grid = {\n",
    "            'C': [0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "            'penalty': ['l1','l2'],\n",
    "            'random_state': [self.random_seed],\n",
    "            'solver':['liblinear'],\n",
    "            'max_iter':[1000],\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, \n",
    "                                   scoring=self.n_weight,\n",
    "                                   cv=cv, n_jobs=-1)\n",
    "        grid_search.fit(self.confounder, self.treatment)\n",
    "        return grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0d8804-adb8-4560-807c-b7736511a367",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to record and write timing information\n",
    "def record_and_write_timing(block_name, start_time):\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    with open(\"/Workspace/Users/qingyuan.song.1@vumc.org/Protein Cancer Risk/timing_info.txt\", \"a\") as file:\n",
    "        file.write(f\"{block_name}: {execution_time} seconds\\n\")\n",
    "\n",
    "def save_output_checkpoint(output_df, output_file_path):\n",
    "    output_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c977c2-d7b2-4e78-be3d-74719c5f2864",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the root folder\n",
    "root_folder = '/Workspace/Users/qingyuan.song.1@vumc.org/Protein Cancer Risk'\n",
    "# Drug information\n",
    "drug_file_path = os.path.join(root_folder, 'protein_supplementary_tables_20240227.xlsx')\n",
    "# Map between ATC code and drug name\n",
    "atc_file_path = os.path.join(root_folder, 'WHO ATC-DDD 2021-12-03.csv')\n",
    "#confounders icd code for each type of cancer\n",
    "confounder_icd_code_file_path = os.path.join(root_folder, 'Cancer_Confounders_with_ICD.csv')\n",
    "\n",
    "# Outputs\n",
    "date_time_suffix = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_file_path = os.path.join(root_folder, f\"output_{date_time_suffix}.csv\")\n",
    "km_curve_file_path = os.path.join(root_folder, f\"KM_curve_{date_time_suffix}.csv\" )\n",
    "\n",
    "\n",
    "n_level = 2\n",
    "random_seed = 37\n",
    "\n",
    "drug_table = pd.read_excel(drug_file_path, sheet_name='S16', skiprows=1)\n",
    "drug_table = drug_table[(drug_table['MaxPhase'] == 4) & (drug_table['IS_APPROVED_AntiCancerDrug'] == False)].reset_index(drop=True)\n",
    "\n",
    "atc_df = pd.read_csv(atc_file_path)\n",
    "confounder_table = pd.read_csv(confounder_icd_code_file_path)\n",
    "\n",
    "cancer_icd_codes_dict = {'BRCA':{'icd10':['C50'], 'icd9':['174', '175']}, 'COADREAD':{'icd10':['C18', 'C19', 'C20'], 'icd9':['153', '154']}, 'PRAD':{'icd10':['C61'], 'icd9':['185']}} \n",
    "\n",
    "km_df = pd.DataFrame(columns=['Cancer type',\n",
    "                              'Treated drug',\n",
    "                              'Controlled drug',\n",
    "                              'Timeline',\n",
    "                              'Treated',\n",
    "                              'Treated_lower_0.95',\n",
    "                              'Treated_upper_0.95',\n",
    "                              'Controlled',\n",
    "                              'Controlled_lower_0.95',\n",
    "                              'Controlled_upper_0.95'])\n",
    "\n",
    "save_output_checkpoint(km_df, km_curve_file_path)\n",
    "\n",
    "output_df = pd.DataFrame(columns=['cancer type',\n",
    "                                  'treated drug generic name',\n",
    "                                  'controlled drug generic name', \n",
    "                                  'unbalanced_covariate_percentage',\n",
    "                                  'weighted KM survival difference',\n",
    "                                  'weighted KM ATE',\n",
    "                                  'weighted KM p value',\n",
    "                                  'weighted CoxPH harzard ratio',\n",
    "                                  'weighted CoxPH standard estimation',\n",
    "                                  'weighted CoxPH confidence intervals lower',\n",
    "                                  'weighted CoxPH confidence intervals upper',\n",
    "                                  'weighted CoxPH P value',\n",
    "                                  'treated group size', \n",
    "                                  'controlled group size',\n",
    "                                  'treated cancer group size',\n",
    "                                  'controlled cancer group size',\n",
    "                                  'LR best hyperparameter'])\n",
    "\n",
    "save_output_checkpoint(output_df, output_file_path)\n",
    "\n",
    "SQLTable(table_name='pat_ingredient', category='drug_count').create_table()\n",
    "pat_ingredient = Dataset.spark_to_df('pat_ingredient')\n",
    "valid_drug = pat_ingredient.loc[pat_ingredient['count_person_id'] >= 500, 'ingredient_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4149b68-f407-4736-a1f6-6135db8ebd8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for index, row in tqdm(drug_table.iterrows(), total=drug_table.shape[0], desc='treated loop'):\n",
    "    cancer_icd_codes = cancer_icd_codes_dict[row['Cancer']]\n",
    "    # Start timing for treated drug\n",
    "    start_time_block1 = time.time()\n",
    "\n",
    "    treated_drug_raw = row['MolecularBrandName']\n",
    "    treated_drug = treated_drug_raw.split(' ')[0]\n",
    "    if valid_drug.str.contains(treated_drug, case=False).sum() == 0:\n",
    "        continue\n",
    "\n",
    "    candidate_controlled_drug = find_similar_atc_codes(treated_drug, n_level, atc_df)\n",
    "    if treated_drug_raw != treated_drug:\n",
    "        add_candidate_controlled_drug = find_similar_atc_codes(treated_drug_raw, n_level, atc_df)\n",
    "        candidate_controlled_drug.extend(add_candidate_controlled_drug)\n",
    "    candidate_controlled_drug = [drug for drug in candidate_controlled_drug if valid_drug.str.contains(drug, case=False).sum() > 0]\n",
    "    if len(candidate_controlled_drug) == 0:\n",
    "        continue\n",
    "\n",
    "    print('treated drug:', treated_drug_raw)\n",
    "    SQLTable(table_name='pat_group2', items=[treated_drug], category='drug').create_table()\n",
    "    pat_case = Dataset.spark_to_df('pat_group2')\n",
    "    pat_case = DataProcessor.aggregate_drug_record(pat_case)\n",
    "\n",
    "    record_and_write_timing(\"Treated patient SQL:\\t\", start_time_block1)\n",
    "\n",
    "    for candidate_drug in tqdm(candidate_controlled_drug, desc='controlled loop', leave=False):\n",
    "\n",
    "        print(candidate_drug)\n",
    "        # Start timing for controlled drug\n",
    "        start_time_block2 = time.time()\n",
    "        controlled_drug = [candidate_drug]\n",
    "\n",
    "        pat_control, pat_record, pat_cancer, pat_demo = Dataset(controlled_drug, cancer_icd_codes).collect_data()\n",
    "        pat_control = DataProcessor.aggregate_drug_record(pat_control)\n",
    "\n",
    "        record_and_write_timing(\"Controlled patient SQL:\\t\", start_time_block2)\n",
    "\n",
    "        # Start timing for processing SQL\n",
    "        start_time_block3 = time.time()\n",
    "        dp = DataProcessor(pat_demo, pat_case, pat_control, pat_cancer, pat_record)\n",
    "        df = dp.merge_data()\n",
    "        df = dp.calculate_variables(df)\n",
    "\n",
    "        df_ = df[(df['time_to_event'] > 0) | (df['time_to_event'].isna())].copy()\n",
    "        df_['time_to_event'] = df_['time_to_event'].fillna(DataProcessor.calculate_time_period(df_, 'record_end_date', 'drug_start_date'))\n",
    "        \n",
    "        df_['gender'] = df_['gender_source_value'].apply(lambda x: int(x == 'M'))\n",
    "\n",
    "        if row['Cancer'] == 'BRCA':\n",
    "            df_ = df_[df_['gender'] == 0].drop('gender', axis=1)\n",
    "        elif row['Cancer'] == 'PRAD':\n",
    "            df_ = df_[df_['gender'] == 1].drop('gender', axis=1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if (df_['is_case'].sum() < 250) or (df_['is_control'].sum() < 250):\n",
    "            continue\n",
    "\n",
    "        df_['ethnicity_Hispanic or Latino'] = (df_['ethnicity_source_value'] == 'Hispanic or Latino').astype(int)\n",
    "        pat_race_d = pd.get_dummies(df_[['person_id','race_source_value']], columns=['race_source_value'], prefix='race')\n",
    "        pat_race_d = pat_race_d[['person_id', 'race_Black or African American', 'race_White']]\n",
    "\n",
    "        record_and_write_timing(\"Processing SQL:\\t\", start_time_block3)\n",
    "        \n",
    "        # Start timing for phecode SQL\n",
    "        start_time_block4 = time.time()\n",
    "\n",
    "        spark_df = spark.createDataFrame(df_[['person_id', 'phecode_end_date']])      \n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"pat_cohort\")\n",
    "\n",
    "        icd9_codes = confounder_table[(confounder_table['Cancer'] == row['Cancer']) | (confounder_table['Cancer'] == 'G')].ICD9.apply(sep_icd_code).to_list()\n",
    "        icd10_codes = confounder_table[(confounder_table['Cancer'] == row['Cancer']) | (confounder_table['Cancer'] == 'G')].ICD10.apply(sep_icd_code).to_list()\n",
    "        icd9_codes = list(set([c for l in icd9_codes for c in l]))\n",
    "        icd10_codes = list(set([c for l in icd10_codes for c in l]))\n",
    "\n",
    "        SQLTable(table_name='pat_phecode', items={'icd9': icd9_codes, 'icd10': icd10_codes}, category='phecode').create_table()\n",
    "        pat_phecode = Dataset.spark_to_df('pat_phecode')\n",
    "\n",
    "        pat_phecode['phecode'] = pat_phecode.phecode.apply(lambda x: x.split('.')[0])\n",
    "        pat_phecode = pat_phecode[['person_id','phecode']].drop_duplicates()\n",
    "\n",
    "        pat_phecode_d = pd.get_dummies(pat_phecode, columns = ['phecode'])\n",
    "        pat_phecode_d = pat_phecode_d.groupby('person_id').agg('sum')\n",
    "\n",
    "        SQLTable(table_name='pat_bmi', category='bmi').create_table()\n",
    "        pat_bmi = Dataset.spark_to_df('pat_bmi')\n",
    "        pat_bmi['bmi'] = pat_bmi['bmi'].astype(float)\n",
    "\n",
    "        if (row['Cancer'] == 'BRCA') or (row['Cancer'] == 'PRAD'):\n",
    "            data = pd.merge(df_[['person_id','age', 'ethnicity_Hispanic or Latino']], pat_race_d, how='inner', on='person_id')\n",
    "        else:\n",
    "            data = pd.merge(df_[['person_id','age','gender', 'ethnicity_Hispanic or Latino']], pat_race_d, how='inner', on='person_id')\n",
    "        data = pd.merge(data, pat_phecode_d.reset_index(), how='inner', on='person_id')\n",
    "        data = pd.merge(data, pat_bmi, how='inner', on='person_id')\n",
    "\n",
    "        data['age'] = (data['age'] - data['age'].min()) / (data['age'].max() - data['age'].min())\n",
    "        data['bmi'] = (data['bmi'] - data['bmi'].min()) / (data['bmi'].max() - data['bmi'].min())\n",
    "        \n",
    "        record_and_write_timing(\"Phecode SQL:\\t\", start_time_block4)\n",
    "\n",
    "        # Start timing for IPTW LR\n",
    "        start_time_block5 = time.time()\n",
    "\n",
    "        data = pd.merge(data, df_[['person_id','is_case','is_cancer','time_to_event']], how='inner', on='person_id')\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        \n",
    "        confounder = data.iloc[:, 1:-3].to_numpy()\n",
    "        treatment = data['is_case'].to_numpy()\n",
    "\n",
    "        lr = PropensityScoreLR(confounder, treatment, random_seed)\n",
    "        best_model, best_params = lr.fit_model()\n",
    "        propensity_score = best_model.predict_proba(confounder)[:, 1]\n",
    "        smd = PropensityScoreLR.cal_SMD(confounder, treatment, propensity_score)\n",
    "        p_unbalanced = len(np.where(smd > 0.1)[0]) / len(smd)\n",
    "\n",
    "        record_and_write_timing(\"IPTW LR:\\t\", start_time_block5)\n",
    "\n",
    "        # Start timing for survival analysis\n",
    "        start_time_block6 = time.time()\n",
    "\n",
    "        duration = data['time_to_event'].to_numpy()\n",
    "\n",
    "        ones_idx, zeros_idx = np.where(treatment == True), np.where(treatment == False)\n",
    "        treated_w, controlled_w = PropensityScoreLR.cal_IPTW(treatment, propensity_score)\n",
    "        treated_duration, controlled_duration = duration[ones_idx], duration[zeros_idx]\n",
    "\n",
    "        cph = CoxPHFitter()\n",
    "\n",
    "        cancer = data['is_cancer'].to_numpy()\n",
    "        treated_cancer, controlled_cancer = cancer[ones_idx], cancer[zeros_idx]\n",
    "\n",
    "        treated_prob = np.matmul(treated_cancer, treated_w) / np.sum(treated_w)\n",
    "        controlled_prob = np.matmul(controlled_cancer, controlled_w) / np.sum(controlled_w)\n",
    "\n",
    "        # CoxPH\n",
    "        weight = np.zeros(len(cancer))\n",
    "        weight[ones_idx] = treated_w.squeeze()\n",
    "        weight[zeros_idx] = controlled_w.squeeze()\n",
    "        cox_data = pd.DataFrame({'T': duration, 'event': cancer, 'treatment': treatment, 'weights':weight})\n",
    "        cox_data = pd.concat([cox_data, data.iloc[:, 1:-3].iloc[:, np.where(smd > 0.1)[0]]], axis=1)\n",
    "\n",
    "        try:\n",
    "            cph.fit(cox_data, duration_col='T', event_col='event', weights_col='weights', robust=True)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # Calculate VIF\n",
    "                vif = pd.DataFrame()\n",
    "                vif[\"Feature\"] = cox_data.columns\n",
    "                vif[\"VIF\"] = [variance_inflation_factor(cox_data, i) for i in range(cox_data.shape[1])]\n",
    "\n",
    "                # Drop columns with VIF > 10\n",
    "                high_vif_features = vif[vif[\"VIF\"] > 10][\"Feature\"]\n",
    "                high_vif_features = [ft for ft in high_vif_features if ft not in ['T','event','weights']]\n",
    "                cox_data_filtered = cox_data.drop(columns=high_vif_features)\n",
    "                cph.fit(cox_data_filtered, duration_col='T', event_col='event', weights_col='weights', robust=True)\n",
    "            except Exception as e:\n",
    "                # Handle Exception, general\n",
    "                print(f\"Exception encountered: {e}. Continuing with next dataset.\")\n",
    "                continue\n",
    "            \n",
    "        kmf_A = KaplanMeierFitter()\n",
    "        kmf_B = KaplanMeierFitter()\n",
    "\n",
    "        # Kaplan Meier\n",
    "        treated_kmf = kmf_A.fit(treated_duration, treated_cancer, label=\"Treated\", weights=treated_w)\n",
    "        controlled_kmf = kmf_B.fit(controlled_duration, controlled_cancer, label=\"Controlled\", weights=controlled_w)\n",
    "        results_w = survival_difference_at_fixed_point_in_time_test([36, 60], treated_kmf, controlled_kmf)\n",
    "        survival_treated = treated_kmf.predict([36, 60]).to_numpy()\n",
    "        survival_controlled = controlled_kmf.predict([36, 60]).to_numpy()\n",
    "        ate_w = survival_treated - survival_controlled\n",
    "        \n",
    "        # Extract the survival function data for both models\n",
    "        treated_survival_df = treated_kmf.survival_function_.rename(columns={'KM_estimate': f'KM_estimate_treated'})\n",
    "        controlled_survival_df = controlled_kmf.survival_function_.rename(columns={'KM_estimate': f'KM_estimate_controlled'})\n",
    "        \n",
    "        # Optionally, include confidence intervals by joining them into the respective DataFrames\n",
    "        treated_survival_df = treated_survival_df.join(treated_kmf.confidence_interval_)\n",
    "        controlled_survival_df = controlled_survival_df.join(controlled_kmf.confidence_interval_)\n",
    "\n",
    "        # Combine the two DataFrames\n",
    "        combined_df = treated_survival_df.join(controlled_survival_df, how='outer').reset_index()\n",
    "        combined_df['cancer type'] = row['Cancer']\n",
    "        combined_df['treated drug'] = treated_drug\n",
    "        combined_df['controlled drug'] = candidate_drug\n",
    "        \n",
    "        km_df = pd.concat([km_df, combined_df], ignore_index=True)\n",
    "\n",
    "        save_output_checkpoint(km_df, km_curve_file_path)\n",
    "\n",
    "        record_and_write_timing(\"Survival analysis\\t\", start_time_block6)\n",
    "\n",
    "        new_row = {'cancer type': row['Cancer'],\n",
    "                'treated drug generic name': treated_drug_raw,\n",
    "                'controlled drug generic name': candidate_drug, \n",
    "                'unbalanced_covariate_percentage': p_unbalanced, \n",
    "                'weighted KM survival difference': results_w.test_statistic.to_dict(),\n",
    "                'weighted KM ATE': ate_w,\n",
    "                'weighted KM p value': results_w.p_value,\n",
    "                'weighted CoxPH harzard ratio': cph.hazard_ratios_['treatment'],\n",
    "                'weighted CoxPH standard estimation': cph.summary.loc['treatment','se(coef)'],\n",
    "                'weighted CoxPH confidence intervals lower': cph.summary.loc['treatment','exp(coef) lower 95%'],\n",
    "                'weighted CoxPH confidence intervals upper': cph.summary.loc['treatment','exp(coef) upper 95%'],\n",
    "                'weighted CoxPH P value': cph.summary.loc['treatment', 'p'],\n",
    "                'treated group size': data['is_case'].sum(), \n",
    "                'controlled group size':(~data['is_case']).sum(),\n",
    "                'treated cancer group size': treated_cancer.sum(),\n",
    "                'controlled cancer group size':controlled_cancer.sum(),\n",
    "                'LR best hyperparameter': best_params,\n",
    "        }\n",
    "        output_df = output_df.append(new_row, ignore_index=True)\n",
    "        save_output_checkpoint(output_df, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a15bf5a8-ab8c-4ef6-87d2-eb2abe958f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Protein cancer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
